{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification via Generative Models\n",
    "(50%) We are going to explore the problem of identifying smartphone position through probabilistic generative models. Motion sensors in smartphones provide valuable information for researchers to understand its owners. An interesting (and more challenging) task is to identify human activities through the data recorded by motion sensors. For example, we want to know whether the smartphone owner is walking, running, or biking. In this homework problem, we are going to tackle a simpler problem. We want to know the static position of the smartphone. There are six possible positions:\n",
    "\n",
    "Phoneonback: The phone is laying on the back of the phone with the screen pointing up (away from the ground).\n",
    "Phoneonfront: The phone is laying on the back of the phone with the screen pointing towards the ground\n",
    "Phoneonbottom: The phone is standing on the bottom of the screen, meaning the bottom is pointed towards the ground\n",
    "Phoneontop: The phone is standing on the top of the screen, meaning the top is pointed towards the ground\n",
    "Phoneonleft: The phone is laying on the left side of the screen.\n",
    "Phoneonright: The phone is laying on the right side of the screen.\n",
    "The input data is the reading of the accelerometer (cf. https://en.wikipedia.org/wiki/Accelerometer) in the smartphone. We have a training dataset that contains about 28,500 data points for phones in each of the six positions. The following is some basic information of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first few rows are:\n",
      "          x         y         z        label\n",
      "0  0.138809  0.074341  9.801056  Phoneonback\n",
      "1  0.164993  0.006500  9.690369  Phoneonback\n",
      "2  0.211411 -0.001831  9.755829  Phoneonback\n",
      "3  0.184036 -0.007782  9.774872  Phoneonback\n",
      "4  0.142380  0.005310  9.765350  Phoneonback\n",
      "\n",
      "Summary statistics:\n",
      "                   x              y              z\n",
      "count  167097.000000  167097.000000  167097.000000\n",
      "mean        0.357340       0.146807      -0.015550\n",
      "std         5.622396       5.552010       5.737150\n",
      "min        -9.770279      -9.966507      -9.908417\n",
      "25%         0.016617      -0.074875      -0.221497\n",
      "50%         0.142776       0.009628       0.025223\n",
      "75%         0.249893       0.295715       0.151032\n",
      "max        10.073685       9.980255      10.031113\n",
      "\n",
      "Label counts:\n",
      "Phoneonleft      29522\n",
      "Phoneonfront     29079\n",
      "Phoneonback      28566\n",
      "Phoneonbottom    27842\n",
      "Phoneontop       26401\n",
      "Phoneonright     25687\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('phone_train.pickle', 'rb') as fh1:\n",
    "    traindata = pickle.load(fh1)\n",
    "    \n",
    "print(\"The first few rows are:\")\n",
    "print(traindata.head())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(traindata.describe())\n",
    "print(\"\\nLabel counts:\")\n",
    "print(traindata['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that the last column, label, is the target of our predictive model and the first three columns are the input features.\n",
    "\n",
    "We are going to train a model that can predict smartphone positions given its accelerometer readings. To achieve this goal, we are going to divide this question into two sub-questions. The first is about training a generative classifier, and the second is to predict smartphone positions.\n",
    "\n",
    "Creat a Python class named mygpc for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1: Create your mypgc class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "class mygpc():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "        self.X_test = None\n",
    "        self.amodel = defaultdict()\n",
    "        self.classes = []\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.X_train = np.array(x)\n",
    "        self.Y_train = np.array(y)\n",
    "        self.train()\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        if self.X_train is None or self.Y_train is None:\n",
    "            raise ValueError(\"Please fit the training data first.\")\n",
    "            \n",
    "        self.classes = np.unique(self.Y_train)\n",
    "        for c in self.classes:\n",
    "            indexes = np.where(self.Y_train == c)\n",
    "            x_in_class = self.X_train[indexes]\n",
    "\n",
    "            self.amodel[c] = dict()\n",
    "            self.amodel[c][\"mu\"] = np.mean(x_in_class, axis=0)\n",
    "            self.amodel[c][\"cov\"] = np.cov(x_in_class, rowvar=False)\n",
    "            \n",
    "            try:\n",
    "                self.amodel[c][\"prec\"] = np.linalg.inv(self.amodel[c][\"cov\"])\n",
    "            except numpy.linalg.LinAlgError:\n",
    "                raise ValueError(\"Covariance Matrix for class {} is not invertible.\".format(c))\n",
    "            \n",
    "            self.amodel[c][\"detcov\"] = math.log(np.linalg.det(self.amodel[c][\"cov\"]))\n",
    "            self.amodel[c][\"n\"] = len(x_in_class)\n",
    "            self.amodel[c][\"prior\"] = len(x_in_class)/len(self.X_train)\n",
    "        \n",
    "        # find Wk and Wk0 for each class k\n",
    "        self.shared_cov = self.ML_cov()\n",
    "        try:\n",
    "            self.shared_cov_inv = np.linalg.inv(self.shared_cov)\n",
    "        except numpy.linalg.LinAlgError:\n",
    "            raise ValueError(\"Shared Covariance Matrix is not invertible.\")\n",
    "        \n",
    "        for c in self.classes:\n",
    "            self.amodel[c][\"W\"] = np.dot(self.shared_cov_inv, self.amodel[c][\"mu\"].transpose())\n",
    "            self.amodel[c][\"W0\"] = np.dot(np.dot(self.amodel[c][\"mu\"], self.shared_cov_inv),\n",
    "                                          self.amodel[c][\"mu\"].transpose())*(-0.5)+math.log(self.amodel[c][\"prior\"])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def show_model_parameters(self):\n",
    "        \n",
    "        np.set_printoptions(precision=3, suppress=True)\n",
    "        \n",
    "        par_names = [\"mu\", \"cov\", \"prec\", \"detcov\", \"n\", \"prior\"]\n",
    "        for c in self.classes:\n",
    "            t = PrettyTable(par_names[:-3]+[\"Others\"])\n",
    "            par_value = []\n",
    "            for par in par_names[:-3]:\n",
    "                par_value.append(self.amodel[c][par])\n",
    "            par_value.append(\"detcov: {}\\nn: {}\\nprior: {}\".format(self.amodel[c][\"detcov\"],\n",
    "                                                                   self.amodel[c][\"n\"],\n",
    "                                                                   self.amodel[c][\"prior\"]))\n",
    "\n",
    "            t.add_row(par_value)\n",
    "            t.align = 'l'\n",
    "            print(c)\n",
    "            print(t)\n",
    "            print()\n",
    "    \n",
    "    def ML_cov(self):\n",
    "        \"\"\"Return Shared Covariance Matrix by Maximum Likelihood.\"\"\"\n",
    "        shared_cov = np.zeros(self.amodel[self.classes[0]][\"cov\"].shape)\n",
    "        for c in self.classes:\n",
    "            shared_cov += (self.amodel[c][\"prior\"] * self.amodel[c][\"cov\"])\n",
    "        \n",
    "        return(shared_cov)\n",
    "            \n",
    "    \n",
    "    def predict_one(self, x_test):\n",
    "        exp_aks = dict()\n",
    "        posteriors = dict()\n",
    "        sum_exp_aks = 0\n",
    "        \n",
    "        best_ak = 0\n",
    "        best_class = str()\n",
    "        for c in self.classes:\n",
    "            ak = np.dot(self.amodel[c][\"W\"], x_test) + self.amodel[c][\"W0\"]\n",
    "            if ak > best_ak:\n",
    "                best_ak = ak\n",
    "                best_class = c\n",
    "\n",
    "#         for c in self.classes:\n",
    "#             exp_ak = math.exp(np.dot(self.amodel[c][\"W\"], x_test) + self.amodel[c][\"W0\"])\n",
    "#             exp_aks[c] = exp_ak\n",
    "#             sum_exp_aks += exp_ak\n",
    "        \n",
    "#         best_posterior = 0\n",
    "#         best_class = \"\"\n",
    "#         for key, value in exp_aks.items():\n",
    "#             posterior = value/sum_exp_aks\n",
    "#             posteriors[key] = posterior\n",
    "#             if posterior > best_posterior:\n",
    "#                 best_posterior = posterior\n",
    "#                 best_class = key\n",
    "        \n",
    "        return best_class\n",
    "            \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        self.X_test = np.array(X_test)\n",
    "        predictions = np.apply_along_axis(self.predict_one, 1, self.X_test)\n",
    "        return predictions\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2: Load data from \"phone_train.picke\" and train your model. List your learned model parameters in a pretty, human readable way so that the TA can easily check the correctness of your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneonback\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                     |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| [ 0.207 -0.026  9.796] | [[ 0.003 -0.002  0.003]  | [[ 943.637  469.548 -316.785]  | detcov: -18.9875162403822  |\n",
      "|                        |  [-0.002  0.002 -0.002]  |  [ 469.548 1146.631  295.426]  | n: 28566                   |\n",
      "|                        |  [ 0.003 -0.002  0.005]] |  [-316.785  295.426  535.768]] | prior: 0.17095459523510295 |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "\n",
      "Phoneonbottom\n",
      "+---------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| mu                  | cov                      | prec                           | Others                      |\n",
      "+---------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| [0.191 9.786 0.147] | [[ 0.003 -0.004  0.002]  | [[1229.249  503.42   -87.227]  | detcov: -18.242307571926013 |\n",
      "|                     |  [-0.004  0.01  -0.003]  |  [ 503.42   374.936  187.616]  | n: 27842                    |\n",
      "|                     |  [ 0.002 -0.003  0.002]] |  [ -87.227  187.616  705.023]] | prior: 0.1666217825574367   |\n",
      "+---------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "\n",
      "Phoneonfront\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                     |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| [ 0.113  0.143 -9.736] | [[ 0.002 -0.006  0.001]  | [[1415.596  258.485   -6.152]  | detcov: -17.33169227717312 |\n",
      "|                        |  [-0.006  0.031 -0.004]  |  [ 258.485   91.321   90.687]  | n: 29079                   |\n",
      "|                        |  [ 0.001 -0.004  0.002]] |  [  -6.152   90.687  729.911]] | prior: 0.1740246683064328  |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "\n",
      "Phoneonleft\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                      |\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| [ 9.926  0.093 -0.019] | [[ 0.002 -0.006  0.001]  | [[1369.951  263.013  -20.496]  | detcov: -17.232141935734493 |\n",
      "|                        |  [-0.006  0.032 -0.003]  |  [ 263.013   88.746   66.699]  | n: 29522                    |\n",
      "|                        |  [ 0.001 -0.003  0.002]] |  [ -20.496   66.699  712.149]] | prior: 0.17667582302494958  |\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "\n",
      "Phoneonright\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                     |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| [-9.649  0.079 -0.008] | [[ 0.001 -0.001  0.001]  | [[1432.025   93.751  -51.149]  | detcov: -18.48369965825519 |\n",
      "|                        |  [-0.001  0.007 -0.006]  |  [  93.751  581.243  463.038]  | n: 25687                   |\n",
      "|                        |  [ 0.001 -0.006  0.008]] |  [ -51.149  463.038  509.368]] | prior: 0.1537250818386925  |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "\n",
      "Phoneontop\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                      |\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| [ 0.001 -9.7   -0.1  ] | [[ 0.002  0.005 -0.006]  | [[1380.201 -186.019   93.798]  | detcov: -17.041318559528136 |\n",
      "|                        |  [ 0.005  0.025 -0.028]  |  [-186.019  629.124  496.622]  | n: 26401                    |\n",
      "|                        |  [-0.006 -0.028  0.033]] |  [  93.798  496.622  465.919]] | prior: 0.15799804903738546  |\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pgc1 = mygpc()\n",
    "pgc1.fit(np.array(traindata[['x', 'y', 'z']]), np.array(traindata['label']))\n",
    "pgc1.show_model_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3: Load the test data from \"phone_test1.pickle\" and apply your predict method. List the first 20 predictions and well as the correct labels. Compute the accuracy for your model. Accuracy is the number of correct predictions divided by total number of data points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 predictions:\n",
      "['Phoneonfront' 'Phoneonbotto' 'Phoneonbotto' 'Phoneonbotto'\n",
      " 'Phoneonfront' 'Phoneonright' 'Phoneonfront' 'Phoneontop' 'Phoneonfront'\n",
      " 'Phoneonfront' 'Phoneonfront' 'Phoneonback' 'Phoneonleft' 'Phoneonback'\n",
      " 'Phoneonbotto' 'Phoneonback' 'Phoneonright' 'Phoneonright' 'Phoneontop'\n",
      " 'Phoneonleft']\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "with open('phone_test1.pickle', 'rb') as fh1:\n",
    "    testdata1 = pickle.load(fh1)\n",
    "\n",
    "predictions = pgc1.predict(np.array(testdata1[['x', 'y', 'z']]))\n",
    "print(\"First 20 predictions:\")\n",
    "print(predictions[:20])\n",
    "\n",
    "test_Y = np.array(testdata1['label'])\n",
    "correct_num = 0\n",
    "for i, predict in enumerate(predictions):\n",
    "    if predict in test_Y[i]:\n",
    "        correct_num += 1\n",
    "        \n",
    "print(\"Accuracy: {}\".format(correct_num/len(predictions)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with L2 Regularization\n",
    "(100%) We discussed the Logistic regression and its variation, Bayesian logistic regression that adopts L2 regularization. The Logistic regression with L2 regularization minimize the following error function (cf. file 0b2 linear model for classification.pdf):\n",
    "\n",
    "λ2wTw−∑ni=1[tnlnyn+(1−tnln(1−yn)], \n",
    "where  yn=11+exp(−wTxn)  and  tn∈{0,1}  is the label value,  xn  is the feature vector, and  w  is the regression coefficient vector.\n",
    "\n",
    "We are going to consider an extension of this model to allow different level of regularization for different regression coefficients. Consider the constant term versus other features. The coefficient of the constant term is usually not regularized in logistic regression. It is because the constant term is associated with the prior class distribution (see the discussion in generative models), and regularizing this term will force the probability of the positive class given a zero feature vector to be 0.5. This may hurt the prediction ability since the true prior class probability may indicate other more reasonable values.\n",
    "\n",
    "Another consideration is regarding the continuous-valued features and binary-valued features. We typically normalize continuous-valued features to have zero means and unit variances but keep binary-value features untouched. It makes sense to have a single regularization value for the continuous-valued features since all of them have been normalized. Similarly, if we do not have additional information, then all binary-valued features can have the same level of regularization. However, using the same regularization coefficient for the continuous-valued and binary-valued features may not be reasonable. That is, it is often beneficial to have a regularization coefficient for the continuous-valued features, and another regularization coefficient for the binary-valued features.\n",
    "\n",
    "The above discussion suggests that a more sophisticated way to regularize a logistic regression is to have three regularization coefficients: 0 for the constant,  a1  for continuous-valued features, and  a2  for the binary-valued features. It is possible to further refine the regularization coefficients. However, hyper-parameter tuning associated with more regularization coefficients may be costly.\n",
    "\n",
    "To achieve this goal, we are going to consider a variation of L2-regularized logistic regression that allow different level of regularization for each coefficient. In the following discussion, we are going to use  X  to denote the feature matrix in the training data. The i-th row in  X ,  xi , is the feature vector for the i-th training data. The last column of  X  is one unless the use does not want to include the constant term.\n",
    "\n",
    "In this model, each regression coefficient may be associated with a different regularization coefficient. Bearing with the risk of ambigulity, we (again) use the scalar  λi  to denote the regularization coefficient for  wi . The vector  w=[w1,w2,...,wD]T  is the regression coefficient vector. Let  Λ  denote the diagonal matrix that have  λi  at  Λii . Our new error function becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1 (20%) Download the Adult dataset. Clean up the dataset and create x_train, y_train, x_test, y_test for training feature, training value, test feature, test label. All of these variables should be numpy arrays. Provide summary statistics for your training and test datasets so that TA can verify the correctness of your procedure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age         workclass  fnlwgt  education education-num      marital-status  \\\n",
       "0  39         State-gov   77516  Bachelors            13       Never-married   \n",
       "1  50  Self-emp-not-inc   83311  Bachelors            13  Married-civ-spouse   \n",
       "2  38           Private  215646    HS-grad             9            Divorced   \n",
       "3  53           Private  234721       11th             7  Married-civ-spouse   \n",
       "4  28           Private  338409  Bachelors            13  Married-civ-spouse   \n",
       "\n",
       "          occupation   relationship   race     sex capital-gain capital-loss  \\\n",
       "0       Adm-clerical  Not-in-family  White    Male         2174            0   \n",
       "1    Exec-managerial        Husband  White    Male            0            0   \n",
       "2  Handlers-cleaners  Not-in-family  White    Male            0            0   \n",
       "3  Handlers-cleaners        Husband  Black    Male            0            0   \n",
       "4     Prof-specialty           Wife  Black  Female            0            0   \n",
       "\n",
       "  hours-per-week native-country  label  \n",
       "0             40  United-States  <=50K  \n",
       "1             13  United-States  <=50K  \n",
       "2             40  United-States  <=50K  \n",
       "3             40  United-States  <=50K  \n",
       "4             40           Cuba  <=50K  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "col_name = [\"age\", \"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\n",
    "           \"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"label\"]\n",
    "\n",
    "res = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\")\n",
    "adult_data = pd.DataFrame(np.genfromtxt(StringIO(res.text), delimiter=', ', dtype='unicode'))\n",
    "adult_data.columns = col_name\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Summary statistics for X_train:\n",
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  30152.000000  3.015200e+04   30152.000000  30152.000000  30152.000000   \n",
      "mean      38.440568  1.897916e+05      10.121319   1092.370025     88.266085   \n",
      "std       13.135362  1.056567e+05       2.550204   7407.547899    404.046306   \n",
      "min       17.000000  1.376900e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.176248e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.784250e+05      10.000000      0.000000      0.000000   \n",
      "75%       47.000000  2.376255e+05      13.000000      0.000000      0.000000   \n",
      "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours-per-week  native-country_Cambodia  native-country_Canada  \\\n",
      "count    30152.000000             30152.000000           30152.000000   \n",
      "mean        40.931348                 0.000597               0.003549   \n",
      "std         11.979776                 0.024426               0.059466   \n",
      "min          1.000000                 0.000000               0.000000   \n",
      "25%         40.000000                 0.000000               0.000000   \n",
      "50%         40.000000                 0.000000               0.000000   \n",
      "75%         45.000000                 0.000000               0.000000   \n",
      "max         99.000000                 1.000000               1.000000   \n",
      "\n",
      "       native-country_China  native-country_Columbia  ...  education_9th  \\\n",
      "count          30152.000000             30152.000000  ...   30152.000000   \n",
      "mean               0.002255                 0.001857  ...       0.015090   \n",
      "std                0.047437                 0.043057  ...       0.121914   \n",
      "min                0.000000                 0.000000  ...       0.000000   \n",
      "25%                0.000000                 0.000000  ...       0.000000   \n",
      "50%                0.000000                 0.000000  ...       0.000000   \n",
      "75%                0.000000                 0.000000  ...       0.000000   \n",
      "max                1.000000                 1.000000  ...       1.000000   \n",
      "\n",
      "       education_Assoc-acdm  education_Assoc-voc  education_Bachelors  \\\n",
      "count          30152.000000         30152.000000         30152.000000   \n",
      "mean               0.033431             0.043347             0.167253   \n",
      "std                0.179761             0.203640             0.373207   \n",
      "min                0.000000             0.000000             0.000000   \n",
      "25%                0.000000             0.000000             0.000000   \n",
      "50%                0.000000             0.000000             0.000000   \n",
      "75%                0.000000             0.000000             0.000000   \n",
      "max                1.000000             1.000000             1.000000   \n",
      "\n",
      "       education_Doctorate  education_HS-grad  education_Masters  \\\n",
      "count         30152.000000       30152.000000       30152.000000   \n",
      "mean              0.012437           0.326214           0.053927   \n",
      "std               0.110827           0.468834           0.225877   \n",
      "min               0.000000           0.000000           0.000000   \n",
      "25%               0.000000           0.000000           0.000000   \n",
      "50%               0.000000           0.000000           0.000000   \n",
      "75%               0.000000           1.000000           0.000000   \n",
      "max               1.000000           1.000000           1.000000   \n",
      "\n",
      "       education_Preschool  education_Prof-school  education_Some-college  \n",
      "count         30152.000000           30152.000000            30152.000000  \n",
      "mean              0.001492               0.017976                0.221378  \n",
      "std               0.038604               0.132865                0.415182  \n",
      "min               0.000000               0.000000                0.000000  \n",
      "25%               0.000000               0.000000                0.000000  \n",
      "50%               0.000000               0.000000                0.000000  \n",
      "75%               0.000000               0.000000                0.000000  \n",
      "max               1.000000               1.000000                1.000000  \n",
      "\n",
      "[8 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "continuos_col_name = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "binary_col_name = list(set(col_name)-set(continuos_col_name))\n",
    "binary_col_name.remove(\"label\")\n",
    "\n",
    "adult_data = adult_data.replace({'?':np.NaN}).dropna()\n",
    "\n",
    "for col in continuos_col_name:\n",
    "    adult_data[col] = pd.to_numeric(adult_data[col])\n",
    "\n",
    "binary_df = pd.DataFrame()\n",
    "for col in binary_col_name:\n",
    "    df = pd.get_dummies(adult_data[col], prefix=col)\n",
    "    binary_df = pd.concat([binary_df, df], axis=1)\n",
    "\n",
    "train_dataset = pd.concat([adult_data[continuos_col_name], binary_df, adult_data[['label']]], axis=1)\n",
    "\n",
    "deleted_feature = dict()\n",
    "for col in binary_df.columns:\n",
    "    if binary_df[col].sum() <= 10:\n",
    "        category = col.split(\"_\")[0]\n",
    "        feature = col.split(\"_\")[1]\n",
    "        if category not in deleted_feature:\n",
    "            deleted_feature[category] = []\n",
    "        deleted_feature[category].append(feature)\n",
    "\n",
    "for cat, feature_list in deleted_feature.items():\n",
    "    for feature in feature_list:\n",
    "        train_dataset = train_dataset[train_dataset[\"{}_{}\".format(cat, feature)] != 1]\n",
    "        train_dataset.drop(\"{}_{}\".format(cat, feature), axis=1, inplace=True)\n",
    "\n",
    "X_train = train_dataset.drop(\"label\", axis=1)\n",
    "Y_train = train_dataset[\"label\"].replace({\">50K\":1, \"<=50K\":0})\n",
    "\n",
    "print(\"\\nSummary statistics for X_train:\")\n",
    "print(X_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\")\n",
    "adult_test = pd.DataFrame(np.genfromtxt(StringIO(res.text.split(\"\\n\",1)[1]), delimiter=', ', dtype='unicode'))\n",
    "adult_test.columns = col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for X_test:\n",
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  15055.000000  1.505500e+04   15055.000000  15055.000000  15055.000000   \n",
      "mean      38.769711  1.896046e+05      10.112255   1120.188907     89.071471   \n",
      "std       13.381046  1.056274e+05       2.558629   7704.274825    406.347469   \n",
      "min       17.000000  1.349200e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.166450e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.779510e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.385810e+05      13.000000      0.000000      0.000000   \n",
      "max       90.000000  1.490400e+06      16.000000  99999.000000   3770.000000   \n",
      "\n",
      "       hours-per-week  native-country_Cambodia  native-country_Canada  \\\n",
      "count    15055.000000             15055.000000           15055.000000   \n",
      "mean        40.950714                 0.000531               0.003720   \n",
      "std         12.064465                 0.023046               0.060878   \n",
      "min          1.000000                 0.000000               0.000000   \n",
      "25%         40.000000                 0.000000               0.000000   \n",
      "50%         40.000000                 0.000000               0.000000   \n",
      "75%         45.000000                 0.000000               0.000000   \n",
      "max         99.000000                 1.000000               1.000000   \n",
      "\n",
      "       native-country_China  native-country_Columbia  ...  education_9th  \\\n",
      "count          15055.000000             15055.000000  ...    15055.00000   \n",
      "mean               0.002989                 0.001727  ...        0.01468   \n",
      "std                0.054592                 0.041523  ...        0.12027   \n",
      "min                0.000000                 0.000000  ...        0.00000   \n",
      "25%                0.000000                 0.000000  ...        0.00000   \n",
      "50%                0.000000                 0.000000  ...        0.00000   \n",
      "75%                0.000000                 0.000000  ...        0.00000   \n",
      "max                1.000000                 1.000000  ...        1.00000   \n",
      "\n",
      "       education_Assoc-acdm  education_Assoc-voc  education_Bachelors  \\\n",
      "count          15055.000000         15055.000000         15055.000000   \n",
      "mean               0.033145             0.043308             0.167785   \n",
      "std                0.179021             0.203556             0.373687   \n",
      "min                0.000000             0.000000             0.000000   \n",
      "25%                0.000000             0.000000             0.000000   \n",
      "50%                0.000000             0.000000             0.000000   \n",
      "75%                0.000000             0.000000             0.000000   \n",
      "max                1.000000             1.000000             1.000000   \n",
      "\n",
      "       education_Doctorate  education_HS-grad  education_Masters  \\\n",
      "count         15055.000000       15055.000000       15055.000000   \n",
      "mean              0.011226           0.328263           0.058851   \n",
      "std               0.105358           0.469597           0.235353   \n",
      "min               0.000000           0.000000           0.000000   \n",
      "25%               0.000000           0.000000           0.000000   \n",
      "50%               0.000000           0.000000           0.000000   \n",
      "75%               0.000000           1.000000           0.000000   \n",
      "max               1.000000           1.000000           1.000000   \n",
      "\n",
      "       education_Preschool  education_Prof-school  education_Some-college  \n",
      "count         15055.000000           15055.000000            15055.000000  \n",
      "mean              0.001793               0.016074                0.213816  \n",
      "std               0.042312               0.125766                0.410012  \n",
      "min               0.000000               0.000000                0.000000  \n",
      "25%               0.000000               0.000000                0.000000  \n",
      "50%               0.000000               0.000000                0.000000  \n",
      "75%               0.000000               0.000000                0.000000  \n",
      "max               1.000000               1.000000                1.000000  \n",
      "\n",
      "[8 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "adult_test = adult_test.replace({'?':np.NaN}).dropna()\n",
    "\n",
    "for col in continuos_col_name:\n",
    "    adult_test[col] = pd.to_numeric(adult_test[col])\n",
    "    \n",
    "test_binary_df = pd.DataFrame()\n",
    "for col in binary_col_name:\n",
    "    df = pd.get_dummies(adult_test[col], prefix=col)\n",
    "    test_binary_df = pd.concat([test_binary_df, df], axis=1)\n",
    "\n",
    "# train_test_diff = set(binary_df.columns).symmetric_difference(set(test_binary_df.columns))\n",
    "# if len(train_test_diff) > 0:\n",
    "#     print(train_test_diff)\n",
    "\n",
    "# found that there is no Holand-Netherlands in testing data\n",
    "    \n",
    "test_dataset = pd.concat([adult_test[continuos_col_name], test_binary_df, adult_test[['label']]], axis=1)\n",
    "\n",
    "for cat, feature_list in deleted_feature.items():\n",
    "    for feature in feature_list:\n",
    "        if feature == \"Holand-Netherlands\":\n",
    "            continue\n",
    "        test_dataset = test_dataset[test_dataset[\"{}_{}\".format(cat, feature)] != 1]\n",
    "        test_dataset.drop(\"{}_{}\".format(cat, feature), axis=1, inplace=True)\n",
    "\n",
    "X_test = test_dataset.drop(\"label\", axis=1).reindex(columns=X_train.columns)\n",
    "Y_test = test_dataset[\"label\"].replace({\">50K.\":1, \"<=50K.\":0})\n",
    "\n",
    "print(\"\\nSummary statistics for X_test:\")\n",
    "print(X_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2 (20%) Derive the gradient and hession matrix for the new E(w).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla E(w) = \\Lambda w + \\Phi ^ T (y-t)$, where y = $\\sigma(w^T \\phi_n)$   \n",
    "$\\nabla \\nabla E(w) = \\Lambda + \\Phi^T R \\Phi$, where R is a diagonal N x N matrix, with $R_(nn) = y_n(1-y_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3 (30%) Create your own mylogistic_l2 class. Show the learned w as well as test accuracy for the cases below. If w is too long for you, show selected w for continuous-valued, binary-valued, and the constant term.**   \n",
    "* Case 1: lambda = 1 for all coefficients   \n",
    "* Case 2: lambda = 1 for all but the intercept, no regularization for incercept term.   \n",
    "* Case 3: lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for incercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class mylogistic_l2():\n",
    "    \n",
    "    def __init__(self, reg_vec:list, max_iter:int, tol, add_intercept:bool):\n",
    "        \n",
    "        self.reg_vec = np.array(reg_vec)\n",
    "        self.reg_matrix = np.diag(self.reg_vec)\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.add_intercept = add_intercept\n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "        \n",
    "        self.w_best = None\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.X_train = x # a N x M matrix\n",
    "        self.Y_train = y.reshape((len(y), 1)) # t is a N x 1 matrix\n",
    "        \n",
    "        if self.add_intercept is True:\n",
    "            const = np.zeros((len(self.X_train), 1)) + 1\n",
    "            self.X_train = np.append(self.X_train, const, axis=1)\n",
    "        \n",
    "        self.train()\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "            return(1 / (1+math.exp(-x)) )\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "            \n",
    "        def get_gradient_hessian(w):\n",
    "            \"\"\"compute the gradient and hessian of the error function with corresponding vector w\"\"\"\n",
    "            \n",
    "            w = w.reshape((len(w), 1)) # w -> M x 1 matrix\n",
    "            pred = np.matmul(self.X_train, w) # (N x M)(M x 1)\n",
    "            pred_sigmoid = np.apply_along_axis(func1d=self.sigmoid, arr=pred, axis=1).reshape((len(pred),1))\n",
    "            \n",
    "            # compute gradient\n",
    "            reg_gradient = np.matmul(self.reg_matrix, w) # (M x M)(M x 1)\n",
    "            err_gradient = np.matmul(np.transpose(self.X_train), pred_sigmoid - self.Y_train) # (M x N)(N x 1)\n",
    "            gradient = reg_gradient + err_gradient #  a (M x 1) matrix\n",
    "            \n",
    "            # compute hessian\n",
    "#             R = np.diag((pred_sigmoid*(1-pred_sigmoid)).flatten())\n",
    "            R = (pred_sigmoid*(1-pred_sigmoid)).flatten()\n",
    "            err_hessian = np.matmul(np.multiply(np.transpose(self.X_train), R), self.X_train)\n",
    "            hessian = self.reg_matrix + err_hessian\n",
    "            \n",
    "            return((gradient, hessian))\n",
    "        \n",
    "        def get_W0():\n",
    "            \"\"\"use the closed-form solution of ridge regression\"\"\"\n",
    "            \n",
    "            b = self.reg_vec.mean()\n",
    "            bI = np.diag(b * np.array([1 for i in range(len(self.reg_vec))]))\n",
    "            \n",
    "            #(X^T x X + bI)\n",
    "            pre = np.matmul(np.transpose(self.X_train), self.X_train) + bI\n",
    "            \n",
    "            try:\n",
    "                pre_inv = np.linalg.inv(pre)\n",
    "            except numpy.linalg.LinAlgError:\n",
    "                raise ValueError(\"Invertible Error while generating W0.\")\n",
    "            \n",
    "            W0 = np.matmul(np.matmul(pre_inv, np.transpose(self.X_train)), self.Y_train)\n",
    "            return(W0)\n",
    "        \n",
    "        def error_function(w):\n",
    "            \"\"\"return the error value with corresponding w\"\"\"\n",
    "            \n",
    "            def one_row_error(x):\n",
    "                t = int(x[-1])\n",
    "                x = x[:-1]\n",
    "\n",
    "                x = x.reshape((len(x), 1))\n",
    "                y = self.sigmoid(np.matmul(np.transpose(w), x))\n",
    "                \n",
    "                return (t*math.log(y) + (1-t)*math.log(1-y))\n",
    "                \n",
    "            w = w.reshape((len(w), 1)) # w -> M x 1 matrix\n",
    "            error_list = np.apply_along_axis(arr=np.append(self.X_train, self.Y_train, axis=1), \n",
    "                                             func1d=one_row_error, axis=1)\n",
    "            \n",
    "            return(error_list.sum() * -1)\n",
    "        \n",
    "        w_old = get_W0()\n",
    "        error_old = error_function(w_old)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            gradient, hessian = get_gradient_hessian(w_old)\n",
    "            try:\n",
    "                hessian_inv = np.linalg.inv(hessian)\n",
    "            except numpy.linalg.LinAlgError:\n",
    "                raise ValueError(\"Invertible Error while inverting hessian on {}th iteration\".format(i+1))\n",
    "            \n",
    "            w_new = w_old - np.matmul(hessian_inv, gradient)\n",
    "            error_new = error_function(w_new)\n",
    "            \n",
    "            if (error_new >= error_old):\n",
    "#                 print(\"Best w found after {}th iteration\".format(i))\n",
    "                break\n",
    "            \n",
    "            w_old = w_new\n",
    "            \n",
    "            if (error_old - error_new) < self.tol:\n",
    "#                 print(\"Best w found after {}th iteration\".format(i+1))\n",
    "                break\n",
    "            \n",
    "            error_old = error_new\n",
    "            \n",
    "        self.w_best = w_old\n",
    "#         print(\"Finish training.\")\n",
    "    \n",
    "    def predict_single(self, x_test):\n",
    "        x_test = x_test.reshape((len(x_test), 1))\n",
    "        c1_probability = self.sigmoid(np.matmul(np.transpose(self.w_best), x_test))\n",
    "        \n",
    "        prediction = 1 if c1_probability > 0.5 else 0\n",
    "        return(prediction)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        self.X_test = X_test\n",
    "        if self.add_intercept is True:\n",
    "            const = np.zeros((len(X_test), 1)) + 1\n",
    "            self.X_test = np.append(self.X_test, const, axis=1)\n",
    "        \n",
    "        predictions = np.apply_along_axis(self.predict_single, 1, self.X_test)\n",
    "        return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>   \n",
    "* Case 1: lambda = 1 for all coefficients  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best w found after 8th iteration\n",
      "Finish training.\n"
     ]
    }
   ],
   "source": [
    "M = X_train.shape[1]\n",
    "lambda_vec = [1 for i in range(M+1)]\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 0.00001, add_intercept = True)\n",
    "logic1.fit(np.array(X_train), np.array(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for case 1: 0.8481567585519761\n"
     ]
    }
   ],
   "source": [
    "predictions_1 = logic1.predict(np.array(X_test))\n",
    "correct_num = 0\n",
    "\n",
    "observations = np.array(Y_test)\n",
    "for i in range(len(Y_test)):\n",
    "    if predictions_1[i] == observations[i]:\n",
    "        correct_num += 1\n",
    "\n",
    "print(\"Accuracy for case 1: {}\".format(correct_num/len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>   \n",
    "* Case 2: lambda = 1 for all but the intercept, no regularization for incercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best w found after 8th iteration\n",
      "Finish training.\n"
     ]
    }
   ],
   "source": [
    "M = X_train.shape[1]\n",
    "lambda_vec = [1 for i in range(M+1)]\n",
    "lambda_vec[-1] = 0\n",
    "logic2 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 0.00001, add_intercept = True)\n",
    "logic2.fit(np.array(X_train), np.array(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for case 2: 0.8478910660909996\n"
     ]
    }
   ],
   "source": [
    "predictions_2 = logic2.predict(np.array(X_test))\n",
    "correct_num = 0\n",
    "\n",
    "observations = np.array(Y_test)\n",
    "for i in range(len(Y_test)):\n",
    "    if predictions_2[i] == observations[i]:\n",
    "        correct_num += 1\n",
    "\n",
    "print(\"Accuracy for case 2: {}\".format(correct_num/len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>   \n",
    "* Case 3:  lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for incercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best w found after 8th iteration\n",
      "Finish training.\n"
     ]
    }
   ],
   "source": [
    "M = X_train.shape[1]\n",
    "lambda_vec = [1 for i in range(M+1)]\n",
    "index = len(continuos_col_name)\n",
    "for i in range(index, len(lambda_vec)):\n",
    "    lambda_vec[i] = 0.5\n",
    "\n",
    "lambda_vec[-1] = 0\n",
    "\n",
    "logic3 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 0.00001, add_intercept = True)\n",
    "logic3.fit(np.array(X_train), np.array(Y_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for case 3: 0.8477582198605115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_3 = logic3.predict(np.array(X_test))\n",
    "correct_num = 0\n",
    "\n",
    "observations = np.array(Y_test)\n",
    "for i in range(len(Y_test)):\n",
    "    if predictions_3[i] == observations[i]:\n",
    "        correct_num += 1\n",
    "\n",
    "print(\"Accuracy for case 3: {}\\n\".format(correct_num/len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.4 (10%) Further split the training data into subtraining (90%) and tuning (10%) to search for the best hyper-parameters. Set the regularization coefficient for the constant term to zero. Allow different regularizations for continuous-valued and binary-valued features. Let  a1  and  a2  denote the regularization coefficients for continuous-valued and binary-valued features. Search the best  a1  and  a2  and report the test accuracy using the best hyper-parameters. You should follow the following procedure to search for the best hyper-parameters.\n",
    "\n",
    "Choose a set of grids among a reasonable range. For example, 10 grids in [0.01, 100].\n",
    "Conduct grid search with the constraint that  a1=a2 . Record the best value  a∗1  and  a∗2 .\n",
    "Fix  a1=a∗1 , and search  a2  for the best value, call the result the new  a∗2 .\n",
    "Fix  a2=a∗2 , and search  a1  for the best value.\n",
    "Report the selected  a1  and  a2 .\n",
    "Train a model using the selected hyper-parameters, and report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best a1=a2= 1.6681005372000592\n",
      "best a2= 1.6681005372000592\n",
      "best a1= 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_lambda_vec(a1, a2, a3=0):\n",
    "    con = [a1 for i in range(len(continuos_col_name))]\n",
    "    bin = [a2 for i in range(X_train.shape[1] - len(continuos_col_name))]\n",
    "    return(con+bin+[a3])\n",
    "\n",
    "def accuracy_score(pred, observ):\n",
    "    correct_num = 0\n",
    "    for i in range(len(observ)):\n",
    "        if pred[i] == observ[i]:\n",
    "            correct_num += 1\n",
    "    return(correct_num/len(observ))\n",
    "\n",
    "X_subtrain, X_tuning, Y_subtrain, Y_tuning = train_test_split(np.array(X_train), np.array(Y_train), test_size=0.1)\n",
    "a_list = np.logspace(-2, 2, 10) \n",
    "#[  0.01 ,   0.028,   0.077,   0.215,   0.599,   1.668,   4.642, 12.915,  35.938, 100. ]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_a = None\n",
    "for a in a_list:\n",
    "    lambda_vec = get_lambda_vec(a,a)\n",
    "    logic = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 0.00001, add_intercept = True)\n",
    "    logic.fit(np.array(X_subtrain), np.array(Y_subtrain))\n",
    "    predictions = logic.predict(np.array(X_tuning))\n",
    "    accuracy = accuracy_score(predictions, np.array(Y_tuning))\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_a = a\n",
    "\n",
    "print(\"best a1=a2= {}\".format(best_a))        \n",
    "        \n",
    "a1 = best_a\n",
    "best_accuracy = 0\n",
    "best_a2 = None\n",
    "for a in a_list:\n",
    "    lambda_vec = get_lambda_vec(a1,a)\n",
    "    logic = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 0.00001, add_intercept = True)\n",
    "    logic.fit(np.array(X_subtrain), np.array(Y_subtrain))\n",
    "    predictions = logic.predict(np.array(X_tuning))\n",
    "    accuracy = accuracy_score(predictions, np.array(Y_tuning))\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_a2 = a\n",
    "\n",
    "print(\"best a2= {}\".format(best_a2))         \n",
    "        \n",
    "best_accuracy = 0\n",
    "best_a1 = None\n",
    "for a in a_list:\n",
    "    lambda_vec = get_lambda_vec(a,best_a2)\n",
    "    logic = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 0.00001, add_intercept = True)\n",
    "    logic.fit(np.array(X_subtrain), np.array(Y_subtrain))\n",
    "    predictions = logic.predict(np.array(X_tuning))\n",
    "    accuracy = accuracy_score(predictions, np.array(Y_tuning))\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_a1 = a\n",
    "\n",
    "print(\"best a1= {}\".format(best_a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for case 4: 0.848090335436732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_vec = get_lambda_vec(best_a1,best_a2)\n",
    "logic4 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 0.00001, add_intercept = True)\n",
    "logic4.fit(np.array(X_train), np.array(Y_train))\n",
    "predictions = logic4.predict(np.array(X_test))\n",
    "accuracy = accuracy_score(predictions, np.array(Y_test))\n",
    "\n",
    "print(\"Accuracy for case 4: {}\\n\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.5 (20%) Use sklearn.linear_model.LogisticRegression to train and test the model (including hyper-parameter tuning). Compare the estimated parameters and test accuracy with those from your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 1.0, 'penalty': 'l1'}\n",
      "accuracy : 0.8474728044574158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from warnings import simplefilter # import warnings filter\n",
    "\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n",
    "# grid={\"C\":np.logspace(-2,2,10), \"solver\":[\"newton-cg\"]}\n",
    "# grid={\"C\":np.logspace(-2,2,10)}\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(np.array(X_train),np.array(Y_train))\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for case 5: 0.8482231816672202\n"
     ]
    }
   ],
   "source": [
    "logreg_official=LogisticRegression(penalty='l1', C=1.0)\n",
    "logreg_official.fit(np.array(X_train),np.array(Y_train))\n",
    "accuracy = logreg_official.score(np.array(X_test), np.array(Y_test))\n",
    "print(\"Accuracy for case 5: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the sklearn model, I get the highest Accuracy 0.84822.   \n",
    "However, the model is using l1 penalty, not l2 penalty like mylogistic_l2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
