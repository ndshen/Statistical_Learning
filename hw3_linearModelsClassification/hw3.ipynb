{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification via Generative Models\n",
    "(50%) We are going to explore the problem of identifying smartphone position through probabilistic generative models. Motion sensors in smartphones provide valuable information for researchers to understand its owners. An interesting (and more challenging) task is to identify human activities through the data recorded by motion sensors. For example, we want to know whether the smartphone owner is walking, running, or biking. In this homework problem, we are going to tackle a simpler problem. We want to know the static position of the smartphone. There are six possible positions:\n",
    "\n",
    "Phoneonback: The phone is laying on the back of the phone with the screen pointing up (away from the ground).\n",
    "Phoneonfront: The phone is laying on the back of the phone with the screen pointing towards the ground\n",
    "Phoneonbottom: The phone is standing on the bottom of the screen, meaning the bottom is pointed towards the ground\n",
    "Phoneontop: The phone is standing on the top of the screen, meaning the top is pointed towards the ground\n",
    "Phoneonleft: The phone is laying on the left side of the screen.\n",
    "Phoneonright: The phone is laying on the right side of the screen.\n",
    "The input data is the reading of the accelerometer (cf. https://en.wikipedia.org/wiki/Accelerometer) in the smartphone. We have a training dataset that contains about 28,500 data points for phones in each of the six positions. The following is some basic information of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first few rows are:\n",
      "          x         y         z        label\n",
      "0  0.138809  0.074341  9.801056  Phoneonback\n",
      "1  0.164993  0.006500  9.690369  Phoneonback\n",
      "2  0.211411 -0.001831  9.755829  Phoneonback\n",
      "3  0.184036 -0.007782  9.774872  Phoneonback\n",
      "4  0.142380  0.005310  9.765350  Phoneonback\n",
      "\n",
      "Summary statistics:\n",
      "                   x              y              z\n",
      "count  167097.000000  167097.000000  167097.000000\n",
      "mean        0.357340       0.146807      -0.015550\n",
      "std         5.622396       5.552010       5.737150\n",
      "min        -9.770279      -9.966507      -9.908417\n",
      "25%         0.016617      -0.074875      -0.221497\n",
      "50%         0.142776       0.009628       0.025223\n",
      "75%         0.249893       0.295715       0.151032\n",
      "max        10.073685       9.980255      10.031113\n",
      "\n",
      "Label counts:\n",
      "Phoneonleft      29522\n",
      "Phoneonfront     29079\n",
      "Phoneonback      28566\n",
      "Phoneonbottom    27842\n",
      "Phoneontop       26401\n",
      "Phoneonright     25687\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('phone_train.pickle', 'rb') as fh1:\n",
    "    traindata = pickle.load(fh1)\n",
    "    \n",
    "print(\"The first few rows are:\")\n",
    "print(traindata.head())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(traindata.describe())\n",
    "print(\"\\nLabel counts:\")\n",
    "print(traindata['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that the last column, label, is the target of our predictive model and the first three columns are the input features.\n",
    "\n",
    "We are going to train a model that can predict smartphone positions given its accelerometer readings. To achieve this goal, we are going to divide this question into two sub-questions. The first is about training a generative classifier, and the second is to predict smartphone positions.\n",
    "\n",
    "Creat a Python class named mygpc for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1: Create your mypgc class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "class mygpc():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "        self.X_test = None\n",
    "        self.amodel = defaultdict()\n",
    "        self.classes = []\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.X_train = np.array(x)\n",
    "        self.Y_train = np.array(y)\n",
    "        self.train()\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        if self.X_train is None or self.Y_train is None:\n",
    "            raise ValueError(\"Please fit the training data first.\")\n",
    "            \n",
    "        self.classes = np.unique(self.Y_train)\n",
    "        for c in self.classes:\n",
    "            indexes = np.where(self.Y_train == c)\n",
    "            x_in_class = self.X_train[indexes]\n",
    "\n",
    "            self.amodel[c]=dict()\n",
    "            self.amodel[c][\"mu\"] = np.mean(x_in_class, axis=0)\n",
    "            self.amodel[c][\"cov\"] = np.cov(x_in_class, rowvar=False)\n",
    "            \n",
    "            try:\n",
    "                self.amodel[c][\"prec\"] = np.linalg.inv(self.amodel[c][\"cov\"])\n",
    "            except numpy.linalg.LinAlgError:\n",
    "                raise ValueError(\"Covariance Matrix for class {} is not invertible.\".format(c))\n",
    "            \n",
    "            self.amodel[c][\"detcov\"] = math.log(np.linalg.det(self.amodel[c][\"cov\"]))\n",
    "            self.amodel[c][\"n\"] = len(x_in_class)\n",
    "            self.amodel[c][\"prior\"] = len(x_in_class)/len(self.X_train)\n",
    "        \n",
    "        # find Wk and Wk0 for each class k\n",
    "        self.shared_cov = self.ML_cov()\n",
    "        try:\n",
    "            self.shared_cov_inv = np.linalg.inv(self.shared_cov)\n",
    "        except numpy.linalg.LinAlgError:\n",
    "            raise ValueError(\"Shared Covariance Matrix is not invertible.\")\n",
    "        \n",
    "        for c in self.classes:\n",
    "            self.amodel[c][\"W\"] = np.dot(self.shared_cov_inv, self.amodel[c][\"mu\"].transpose())\n",
    "            self.amodel[c][\"W0\"] = np.dot(np.dot(self.amodel[c][\"mu\"], self.shared_cov_inv),\n",
    "                                          self.amodel[c][\"mu\"].transpose())*(-0.5)+math.log(self.amodel[c][\"prior\"])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def show_model_parameters(self):\n",
    "        \n",
    "        np.set_printoptions(precision=3, suppress=True)\n",
    "        \n",
    "        par_names = [\"mu\", \"cov\", \"prec\", \"detcov\", \"n\", \"prior\"]\n",
    "        for c in self.classes:\n",
    "            t = PrettyTable(par_names[:-3]+[\"Others\"])\n",
    "            par_value = []\n",
    "            for par in par_names[:-3]:\n",
    "                par_value.append(self.amodel[c][par])\n",
    "            par_value.append(\"detcov: {}\\nn: {}\\nprior: {}\".format(self.amodel[c][\"detcov\"],\n",
    "                                                                   self.amodel[c][\"n\"],\n",
    "                                                                   self.amodel[c][\"prior\"]))\n",
    "\n",
    "            t.add_row(par_value)\n",
    "            t.align = 'l'\n",
    "            print(c)\n",
    "            print(t)\n",
    "            print()\n",
    "    \n",
    "    def ML_cov(self):\n",
    "        \"\"\"Return Shared Covariance Matrix by Maximum Likelihood.\"\"\"\n",
    "        shared_cov = np.zeros(self.amodel[self.classes[0]][\"cov\"].shape)\n",
    "        for c in self.classes:\n",
    "            shared_cov += (self.amodel[c][\"prior\"] * self.amodel[c][\"cov\"])\n",
    "        \n",
    "        return(shared_cov)\n",
    "            \n",
    "    \n",
    "    def predict_one(self, x_test):\n",
    "        exp_aks = dict()\n",
    "        posteriors = dict()\n",
    "        sum_exp_aks = 0\n",
    "        \n",
    "        best_ak = 0\n",
    "        best_class = str()\n",
    "        for c in self.classes:\n",
    "            ak = np.dot(self.amodel[c][\"W\"], x_test) + self.amodel[c][\"W0\"]\n",
    "            if ak > best_ak:\n",
    "                best_ak = ak\n",
    "                best_class = c\n",
    "\n",
    "#         for c in self.classes:\n",
    "#             exp_ak = math.exp(np.dot(self.amodel[c][\"W\"], x_test) + self.amodel[c][\"W0\"])\n",
    "#             exp_aks[c] = exp_ak\n",
    "#             sum_exp_aks += exp_ak\n",
    "        \n",
    "#         best_posterior = 0\n",
    "#         best_class = \"\"\n",
    "#         for key, value in exp_aks.items():\n",
    "#             posterior = value/sum_exp_aks\n",
    "#             posteriors[key] = posterior\n",
    "#             if posterior > best_posterior:\n",
    "#                 best_posterior = posterior\n",
    "#                 best_class = key\n",
    "        \n",
    "        return best_class\n",
    "            \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        self.X_test = np.array(X_test)\n",
    "        predictions = np.apply_along_axis(self.predict_one, 1, self.X_test)\n",
    "        return predictions\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2: Load data from \"phone_train.picke\" and train your model. List your learned model parameters in a pretty, human readable way so that the TA can easily check the correctness of your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneonback\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                     |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| [ 0.207 -0.026  9.796] | [[ 0.003 -0.002  0.003]  | [[ 943.637  469.548 -316.785]  | detcov: -18.9875162403822  |\n",
      "|                        |  [-0.002  0.002 -0.002]  |  [ 469.548 1146.631  295.426]  | n: 28566                   |\n",
      "|                        |  [ 0.003 -0.002  0.005]] |  [-316.785  295.426  535.768]] | prior: 0.17095459523510295 |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "\n",
      "Phoneonbottom\n",
      "+---------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| mu                  | cov                      | prec                           | Others                      |\n",
      "+---------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| [0.191 9.786 0.147] | [[ 0.003 -0.004  0.002]  | [[1229.249  503.42   -87.227]  | detcov: -18.242307571926013 |\n",
      "|                     |  [-0.004  0.01  -0.003]  |  [ 503.42   374.936  187.616]  | n: 27842                    |\n",
      "|                     |  [ 0.002 -0.003  0.002]] |  [ -87.227  187.616  705.023]] | prior: 0.1666217825574367   |\n",
      "+---------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "\n",
      "Phoneonfront\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                     |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| [ 0.113  0.143 -9.736] | [[ 0.002 -0.006  0.001]  | [[1415.596  258.485   -6.152]  | detcov: -17.33169227717312 |\n",
      "|                        |  [-0.006  0.031 -0.004]  |  [ 258.485   91.321   90.687]  | n: 29079                   |\n",
      "|                        |  [ 0.001 -0.004  0.002]] |  [  -6.152   90.687  729.911]] | prior: 0.1740246683064328  |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "\n",
      "Phoneonleft\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                      |\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| [ 9.926  0.093 -0.019] | [[ 0.002 -0.006  0.001]  | [[1369.951  263.013  -20.496]  | detcov: -17.232141935734493 |\n",
      "|                        |  [-0.006  0.032 -0.003]  |  [ 263.013   88.746   66.699]  | n: 29522                    |\n",
      "|                        |  [ 0.001 -0.003  0.002]] |  [ -20.496   66.699  712.149]] | prior: 0.17667582302494958  |\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "\n",
      "Phoneonright\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                     |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "| [-9.649  0.079 -0.008] | [[ 0.001 -0.001  0.001]  | [[1432.025   93.751  -51.149]  | detcov: -18.48369965825519 |\n",
      "|                        |  [-0.001  0.007 -0.006]  |  [  93.751  581.243  463.038]  | n: 25687                   |\n",
      "|                        |  [ 0.001 -0.006  0.008]] |  [ -51.149  463.038  509.368]] | prior: 0.1537250818386925  |\n",
      "+------------------------+--------------------------+--------------------------------+----------------------------+\n",
      "\n",
      "Phoneontop\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| mu                     | cov                      | prec                           | Others                      |\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "| [ 0.001 -9.7   -0.1  ] | [[ 0.002  0.005 -0.006]  | [[1380.201 -186.019   93.798]  | detcov: -17.041318559528136 |\n",
      "|                        |  [ 0.005  0.025 -0.028]  |  [-186.019  629.124  496.622]  | n: 26401                    |\n",
      "|                        |  [-0.006 -0.028  0.033]] |  [  93.798  496.622  465.919]] | prior: 0.15799804903738546  |\n",
      "+------------------------+--------------------------+--------------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pgc1 = mygpc()\n",
    "pgc1.fit(np.array(traindata[['x', 'y', 'z']]), np.array(traindata['label']))\n",
    "pgc1.show_model_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3: Load the test data from \"phone_test1.pickle\" and apply your predict method. List the first 20 predictions and well as the correct labels. Compute the accuracy for your model. Accuracy is the number of correct predictions divided by total number of data points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 predictions:\n",
      "['Phoneonfront' 'Phoneonbotto' 'Phoneonbotto' 'Phoneonbotto'\n",
      " 'Phoneonfront' 'Phoneonright' 'Phoneonfront' 'Phoneontop' 'Phoneonfront'\n",
      " 'Phoneonfront' 'Phoneonfront' 'Phoneonback' 'Phoneonleft' 'Phoneonback'\n",
      " 'Phoneonbotto' 'Phoneonback' 'Phoneonright' 'Phoneonright' 'Phoneontop'\n",
      " 'Phoneonleft']\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "with open('phone_test1.pickle', 'rb') as fh1:\n",
    "    testdata1 = pickle.load(fh1)\n",
    "\n",
    "predictions = pgc1.predict(np.array(testdata1[['x', 'y', 'z']]))\n",
    "print(\"First 20 predictions:\")\n",
    "print(predictions[:20])\n",
    "\n",
    "test_Y = np.array(testdata1['label'])\n",
    "correct_num = 0\n",
    "for i, predict in enumerate(predictions):\n",
    "    if predict in test_Y[i]:\n",
    "        correct_num += 1\n",
    "        \n",
    "print(\"Accuracy: {}\".format(correct_num/len(predictions)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with L2 Regularization\n",
    "(100%) We discussed the Logistic regression and its variation, Bayesian logistic regression that adopts L2 regularization. The Logistic regression with L2 regularization minimize the following error function (cf. file 0b2 linear model for classification.pdf):\n",
    "\n",
    "λ2wTw−∑ni=1[tnlnyn+(1−tnln(1−yn)], \n",
    "where  yn=11+exp(−wTxn)  and  tn∈{0,1}  is the label value,  xn  is the feature vector, and  w  is the regression coefficient vector.\n",
    "\n",
    "We are going to consider an extension of this model to allow different level of regularization for different regression coefficients. Consider the constant term versus other features. The coefficient of the constant term is usually not regularized in logistic regression. It is because the constant term is associated with the prior class distribution (see the discussion in generative models), and regularizing this term will force the probability of the positive class given a zero feature vector to be 0.5. This may hurt the prediction ability since the true prior class probability may indicate other more reasonable values.\n",
    "\n",
    "Another consideration is regarding the continuous-valued features and binary-valued features. We typically normalize continuous-valued features to have zero means and unit variances but keep binary-value features untouched. It makes sense to have a single regularization value for the continuous-valued features since all of them have been normalized. Similarly, if we do not have additional information, then all binary-valued features can have the same level of regularization. However, using the same regularization coefficient for the continuous-valued and binary-valued features may not be reasonable. That is, it is often beneficial to have a regularization coefficient for the continuous-valued features, and another regularization coefficient for the binary-valued features.\n",
    "\n",
    "The above discussion suggests that a more sophisticated way to regularize a logistic regression is to have three regularization coefficients: 0 for the constant,  a1  for continuous-valued features, and  a2  for the binary-valued features. It is possible to further refine the regularization coefficients. However, hyper-parameter tuning associated with more regularization coefficients may be costly.\n",
    "\n",
    "To achieve this goal, we are going to consider a variation of L2-regularized logistic regression that allow different level of regularization for each coefficient. In the following discussion, we are going to use  X  to denote the feature matrix in the training data. The i-th row in  X ,  xi , is the feature vector for the i-th training data. The last column of  X  is one unless the use does not want to include the constant term.\n",
    "\n",
    "In this model, each regression coefficient may be associated with a different regularization coefficient. Bearing with the risk of ambigulity, we (again) use the scalar  λi  to denote the regularization coefficient for  wi . The vector  w=[w1,w2,...,wD]T  is the regression coefficient vector. Let  Λ  denote the diagonal matrix that have  λi  at  Λii . Our new error function becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1 (20%) Download the Adult dataset. Clean up the dataset and create x_train, y_train, x_test, y_test for training feature, training value, test feature, test label. All of these variables should be numpy arrays. Provide summary statistics for your training and test datasets so that TA can verify the correctness of your procedure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age         workclass  fnlwgt  education education-num      marital-status  \\\n",
       "0  39         State-gov   77516  Bachelors            13       Never-married   \n",
       "1  50  Self-emp-not-inc   83311  Bachelors            13  Married-civ-spouse   \n",
       "2  38           Private  215646    HS-grad             9            Divorced   \n",
       "3  53           Private  234721       11th             7  Married-civ-spouse   \n",
       "4  28           Private  338409  Bachelors            13  Married-civ-spouse   \n",
       "\n",
       "          occupation   relationship   race     sex capital-gain capital-loss  \\\n",
       "0       Adm-clerical  Not-in-family  White    Male         2174            0   \n",
       "1    Exec-managerial        Husband  White    Male            0            0   \n",
       "2  Handlers-cleaners  Not-in-family  White    Male            0            0   \n",
       "3  Handlers-cleaners        Husband  Black    Male            0            0   \n",
       "4     Prof-specialty           Wife  Black  Female            0            0   \n",
       "\n",
       "  hours-per-week native-country  label  \n",
       "0             40  United-States  <=50K  \n",
       "1             13  United-States  <=50K  \n",
       "2             40  United-States  <=50K  \n",
       "3             40  United-States  <=50K  \n",
       "4             40           Cuba  <=50K  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "col_name = [\"age\", \"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\n",
    "           \"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"label\"]\n",
    "\n",
    "res = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\")\n",
    "adult_data = pd.DataFrame(np.genfromtxt(StringIO(res.text), delimiter=', ', dtype='unicode'))\n",
    "adult_data.columns = col_name\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for X_train:\n",
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  30152.000000  3.015200e+04   30152.000000  30152.000000  30152.000000   \n",
      "mean      38.440568  1.897916e+05      10.121319   1092.370025     88.266085   \n",
      "std       13.135362  1.056567e+05       2.550204   7407.547899    404.046306   \n",
      "min       17.000000  1.376900e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.176248e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.784250e+05      10.000000      0.000000      0.000000   \n",
      "75%       47.000000  2.376255e+05      13.000000      0.000000      0.000000   \n",
      "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours-per-week  race_Amer-Indian-Eskimo  race_Asian-Pac-Islander  \\\n",
      "count    30152.000000             30152.000000             30152.000000   \n",
      "mean        40.931348                 0.009452                 0.029683   \n",
      "std         11.979776                 0.096763                 0.169714   \n",
      "min          1.000000                 0.000000                 0.000000   \n",
      "25%         40.000000                 0.000000                 0.000000   \n",
      "50%         40.000000                 0.000000                 0.000000   \n",
      "75%         45.000000                 0.000000                 0.000000   \n",
      "max         99.000000                 1.000000                 1.000000   \n",
      "\n",
      "         race_Black    race_Other  ...  workclass_Self-emp-not-inc  \\\n",
      "count  30152.000000  30152.000000  ...                30152.000000   \n",
      "mean       0.093393      0.007661  ...                    0.082880   \n",
      "std        0.290988      0.087194  ...                    0.275705   \n",
      "min        0.000000      0.000000  ...                    0.000000   \n",
      "25%        0.000000      0.000000  ...                    0.000000   \n",
      "50%        0.000000      0.000000  ...                    0.000000   \n",
      "75%        0.000000      0.000000  ...                    0.000000   \n",
      "max        1.000000      1.000000  ...                    1.000000   \n",
      "\n",
      "       workclass_State-gov  workclass_Without-pay  marital-status_Divorced  \\\n",
      "count         30152.000000           30152.000000             30152.000000   \n",
      "mean              0.042418               0.000464                 0.139759   \n",
      "std               0.201545               0.021543                 0.346742   \n",
      "min               0.000000               0.000000                 0.000000   \n",
      "25%               0.000000               0.000000                 0.000000   \n",
      "50%               0.000000               0.000000                 0.000000   \n",
      "75%               0.000000               0.000000                 0.000000   \n",
      "max               1.000000               1.000000                 1.000000   \n",
      "\n",
      "       marital-status_Married-AF-spouse  marital-status_Married-civ-spouse  \\\n",
      "count                      30152.000000                       30152.000000   \n",
      "mean                           0.000696                           0.466370   \n",
      "std                            0.026382                           0.498876   \n",
      "min                            0.000000                           0.000000   \n",
      "25%                            0.000000                           0.000000   \n",
      "50%                            0.000000                           0.000000   \n",
      "75%                            0.000000                           1.000000   \n",
      "max                            1.000000                           1.000000   \n",
      "\n",
      "       marital-status_Married-spouse-absent  marital-status_Never-married  \\\n",
      "count                          30152.000000                  30152.000000   \n",
      "mean                               0.012271                      0.322334   \n",
      "std                                0.110095                      0.467378   \n",
      "min                                0.000000                      0.000000   \n",
      "25%                                0.000000                      0.000000   \n",
      "50%                                0.000000                      0.000000   \n",
      "75%                                0.000000                      1.000000   \n",
      "max                                1.000000                      1.000000   \n",
      "\n",
      "       marital-status_Separated  marital-status_Widowed  \n",
      "count              30152.000000            30152.000000  \n",
      "mean                   0.031142                0.027428  \n",
      "std                    0.173705                0.163329  \n",
      "min                    0.000000                0.000000  \n",
      "25%                    0.000000                0.000000  \n",
      "50%                    0.000000                0.000000  \n",
      "75%                    0.000000                0.000000  \n",
      "max                    1.000000                1.000000  \n",
      "\n",
      "[8 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "continuos_col_name = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "binary_col_name = list(set(col_name)-set(continuos_col_name))\n",
    "binary_col_name.remove(\"label\")\n",
    "\n",
    "adult_data = adult_data.replace({'?':np.NaN}).dropna()\n",
    "\n",
    "for col in continuos_col_name:\n",
    "    adult_data[col] = pd.to_numeric(adult_data[col])\n",
    "\n",
    "binary_df = pd.DataFrame()\n",
    "for col in binary_col_name:\n",
    "    df = pd.get_dummies(adult_data[col], prefix=col)\n",
    "    binary_df = pd.concat([binary_df, df], axis=1)\n",
    "\n",
    "train_dataset = pd.concat([adult_data[continuos_col_name], binary_df, adult_data[['label']]], axis=1)\n",
    "\n",
    "deleted_feature = dict()\n",
    "for col in binary_df.columns:\n",
    "    if binary_df[col].sum() <= 10:\n",
    "        category = col.split(\"_\")[0]\n",
    "        feature = col.split(\"_\")[1]\n",
    "        if category not in deleted_feature:\n",
    "            deleted_feature[category] = []\n",
    "        deleted_feature[category].append(feature)\n",
    "\n",
    "for cat, feature_list in deleted_feature.items():\n",
    "    for feature in feature_list:\n",
    "        train_dataset = train_dataset[train_dataset[\"{}_{}\".format(cat, feature)] != 1]\n",
    "        train_dataset.drop(\"{}_{}\".format(cat, feature), axis=1, inplace=True)\n",
    "\n",
    "X_train = train_dataset.drop(\"label\", axis=1)\n",
    "Y_train = train_dataset[\"label\"].replace({\">50K\":1, \"<=50K\":0})\n",
    "\n",
    "print(\"\\nSummary statistics for X_train:\")\n",
    "print(X_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\")\n",
    "adult_test = pd.DataFrame(np.genfromtxt(StringIO(res.text.split(\"\\n\",1)[1]), delimiter=', ', dtype='unicode'))\n",
    "adult_test.columns = col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for X_test:\n",
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  15055.000000  1.505500e+04   15055.000000  15055.000000  15055.000000   \n",
      "mean      38.769711  1.896046e+05      10.112255   1120.188907     89.071471   \n",
      "std       13.381046  1.056274e+05       2.558629   7704.274825    406.347469   \n",
      "min       17.000000  1.349200e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.166450e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.779510e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.385810e+05      13.000000      0.000000      0.000000   \n",
      "max       90.000000  1.490400e+06      16.000000  99999.000000   3770.000000   \n",
      "\n",
      "       hours-per-week  race_Amer-Indian-Eskimo  race_Asian-Pac-Islander  \\\n",
      "count    15055.000000             15055.000000             15055.000000   \n",
      "mean        40.950714                 0.009897                 0.027101   \n",
      "std         12.064465                 0.098994                 0.162382   \n",
      "min          1.000000                 0.000000                 0.000000   \n",
      "25%         40.000000                 0.000000                 0.000000   \n",
      "50%         40.000000                 0.000000                 0.000000   \n",
      "75%         45.000000                 0.000000                 0.000000   \n",
      "max         99.000000                 1.000000                 1.000000   \n",
      "\n",
      "         race_Black    race_Other  ...  workclass_Self-emp-not-inc  \\\n",
      "count  15055.000000  15055.000000  ...                15055.000000   \n",
      "mean       0.093723      0.008104  ...                    0.086151   \n",
      "std        0.291453      0.089658  ...                    0.280596   \n",
      "min        0.000000      0.000000  ...                    0.000000   \n",
      "25%        0.000000      0.000000  ...                    0.000000   \n",
      "50%        0.000000      0.000000  ...                    0.000000   \n",
      "75%        0.000000      0.000000  ...                    0.000000   \n",
      "max        1.000000      1.000000  ...                    1.000000   \n",
      "\n",
      "       workclass_State-gov  workclass_Without-pay  marital-status_Divorced  \\\n",
      "count         15055.000000           15055.000000             15055.000000   \n",
      "mean              0.044304               0.000465                 0.138359   \n",
      "std               0.205777               0.021559                 0.345288   \n",
      "min               0.000000               0.000000                 0.000000   \n",
      "25%               0.000000               0.000000                 0.000000   \n",
      "50%               0.000000               0.000000                 0.000000   \n",
      "75%               0.000000               0.000000                 0.000000   \n",
      "max               1.000000               1.000000                 1.000000   \n",
      "\n",
      "       marital-status_Married-AF-spouse  marital-status_Married-civ-spouse  \\\n",
      "count                      15055.000000                       15055.000000   \n",
      "mean                           0.000731                           0.464098   \n",
      "std                            0.027022                           0.498726   \n",
      "min                            0.000000                           0.000000   \n",
      "25%                            0.000000                           0.000000   \n",
      "50%                            0.000000                           0.000000   \n",
      "75%                            0.000000                           1.000000   \n",
      "max                            1.000000                           1.000000   \n",
      "\n",
      "       marital-status_Married-spouse-absent  marital-status_Never-married  \\\n",
      "count                          15055.000000                  15055.000000   \n",
      "mean                               0.012089                      0.323481   \n",
      "std                                0.109287                      0.467820   \n",
      "min                                0.000000                      0.000000   \n",
      "25%                                0.000000                      0.000000   \n",
      "50%                                0.000000                      0.000000   \n",
      "75%                                0.000000                      1.000000   \n",
      "max                                1.000000                      1.000000   \n",
      "\n",
      "       marital-status_Separated  marital-status_Widowed  \n",
      "count              15055.000000            15055.000000  \n",
      "mean                   0.031352                0.029890  \n",
      "std                    0.174272                0.170291  \n",
      "min                    0.000000                0.000000  \n",
      "25%                    0.000000                0.000000  \n",
      "50%                    0.000000                0.000000  \n",
      "75%                    0.000000                0.000000  \n",
      "max                    1.000000                1.000000  \n",
      "\n",
      "[8 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "adult_test = adult_test.replace({'?':np.NaN}).dropna()\n",
    "\n",
    "for col in continuos_col_name:\n",
    "    adult_test[col] = pd.to_numeric(adult_test[col])\n",
    "    \n",
    "test_binary_df = pd.DataFrame()\n",
    "for col in binary_col_name:\n",
    "    df = pd.get_dummies(adult_test[col], prefix=col)\n",
    "    test_binary_df = pd.concat([test_binary_df, df], axis=1)\n",
    "\n",
    "# train_test_diff = set(binary_df.columns).symmetric_difference(set(test_binary_df.columns))\n",
    "# if len(train_test_diff) > 0:\n",
    "#     print(train_test_diff)\n",
    "\n",
    "# found that there is no Holand-Netherlands in testing data\n",
    "    \n",
    "test_dataset = pd.concat([adult_test[continuos_col_name], test_binary_df, adult_test[['label']]], axis=1)\n",
    "\n",
    "for cat, feature_list in deleted_feature.items():\n",
    "    for feature in feature_list:\n",
    "        if feature == \"Holand-Netherlands\":\n",
    "            continue\n",
    "        test_dataset = test_dataset[test_dataset[\"{}_{}\".format(cat, feature)] != 1]\n",
    "        test_dataset.drop(\"{}_{}\".format(cat, feature), axis=1, inplace=True)\n",
    "\n",
    "X_test = test_dataset.drop(\"label\", axis=1).reindex(columns=X_train.columns)\n",
    "Y_test = test_dataset[\"label\"].replace({\">50K.\":1, \"<=50K.\":0})\n",
    "\n",
    "print(\"\\nSummary statistics for X_test:\")\n",
    "print(X_test.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
